% !TeX root = main.tex
%!TeX spellcheck = en-US
\documentclass[11pt]{article}
\usepackage{amsmath,amssymb}
\usepackage{supertabular}  
\usepackage{float}
% \usepackage[  
%      includeheadfoot, head=13pt, foot=2pc,
%      paperwidth=6.75in, paperheight=10in,
%      top=58pt, bottom=44pt, inner=46pt, outer=46pt,
%      marginparwidth=2pc,heightrounded
% ]{geometry}  
\usepackage{geometry}
\usepackage{ifthen}  
% \usepackage{pdflscape}   
% \usepackage{alltt}%hack 
% \geometry{a4paper,dvips,twoside,left=22.5mm,right=22.5mm,top=20mm,bottom=30mm}
\usepackage{color}    
\usepackage[dvipsnames]{xcolor}
\newcommand{\blue}[1]{ {\color{RoyalBlue}{{#1}}} }
\newcommand{\black}[1]{ {\color{Black}{{#1}}} }
\newcommand{\red}[1]{ {\color{BrickRed}{{#1}}} }




\usepackage{mathpartir}   
\usepackage{stmaryrd}   

\usepackage{libertine} 
\usepackage{inconsolata}
% \usepackage{libertinust1math}

\usepackage{keyval}
\usepackage{ifthen}  
\usepackage{enumitem}   

\usepackage{amsthm} 
\usepackage{hyperref} 
\usepackage{natbib}   
\bibliographystyle{unsrtnat}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newcommand*{\lemmaautorefname}{Lemma}

\newcommand{\abe}{\ensuremath{\alpha\beta\eta}}
 
\usepackage[implicitPremiseBreaks]{ottalt}  
\newcommand{\targetstyle}[3]{\color{BrickRed}\mathrm{ {#1}_{#2}#3 } }
\newcommand{\sourcestyle}[3]{\color{RoyalBlue}\mathsf{ {#1}_{#2}#3 } }
\newNTclass{source} 

\newsource[\NTCAPTURELOW{\sourcestyle }]{e}{e}  
\newsource[\NTCAPTURELOW{\sourcestyle }]{r}{r}  
\newsource[\NTCAPTURELOW{\sourcestyle }]{v}{v}
\newsource[\NTCAPTURELOW{\sourcestyle }]{T}{\tau}
\newsource[\NTCAPTURELOW{\sourcestyle }]{ep}{\varepsilon}
\newsource[\NTCAPTURELOW{\sourcestyle }]{Gamma}{\Gamma} 

\newNTclass{Ltarget}
\newLtarget[\NTCAPTURELOW{\targetstyle }]{u}{u}
\newLtarget[\NTCAPTURELOW{\targetstyle }]{k}{k}
\newLtarget[\NTCAPTURELOW{\targetstyle }]{d}{d}
\newLtarget[\NTCAPTURELOW{\targetstyle }]{t}{t} 

\inputott{GTFL_defns}       
 
\newcommand{\rrule}[1]{\rref*{#1}}


 

\begin{document}

\section{Introduction}

Gradually typed languages

\section{Source Language Syntax}

We take as our source language GTFL, a gradually-typed functional language
with \textit{evidence}. Such a language is used as the result of
elaboration in the framework of Abstracting Gradual Typing (AGT)~\citep{agt},
and allows the meaning of gradual programs to be determined in terms of evidence.

\subsection{Terms}
 
The syntax for terms for GTFL is given in \autoref{fig:term-syntax}.
The language is essentially a simply typed lambda calculus with
integers, booleans, and base types, except that a term $[[e]]$ 
may be ascribed with \textit{evidence} $[[epsilon]]$.
This evidence contains typing information that evolves throughout the run of a program.
We explain evidence in detail in \autoref{subsec:type-rules}.
Because gradual typing may result in dynamic type errors, we have a special failure term $[[error]]$.

Values are defined in the usual way, except that we do not allow a value to be
ascribed multiple pieces of evidence. To enforce this syntactically, we separate \textit{raw values}
(using the metavariable $[[r]]$), which do not contain evidence at the top-level,
from general values (metavariable $[[v]]$), which ascribe a raw value with zero or one pieces of evidence.

While the original presentation of AGT treated terms as intrinsically typed values,
we adopt the simpler approach used by \citet{Toro:2019:GPR:3302515.3290330}, where evidence ascription
is included in the syntax for terms. 

\begin{figure}[H] 
	$[[n]] \in \mathbb{Z}$, $[[b]]\in\mathbb{B}$

	\nonterms{e}
	\nonterms{v}
	\nonterms{r}
	\caption{Source Language Syntax: Terms}
	\label{fig:term-syntax}
\end{figure}

\subsection{Types}
\label{subsec:source-types}

As a gradually-typed language, the interesting features of GTFL are in its type system.
The syntax for types, given in \autoref{fig:type-syntax}, matches what one expects in a simply-typed calculus, except that
we have also introduced the \textit{unknown} or \textit{dynamic} type $[[?]]$.
Any term can have be assigned type $[[?]]$, and a term of type $[[?]]$ can be used in
any context without being rejected as ill-typed.

To define our typing rules in \autoref{subsec:type-rules}, we need \textit{contexts}, which
assign types to free program variables. Having types also allows us to precisely define what evidence is:
each piece of evidence is simply a type. For the term $[[ep e]]$, $[[ep]]$ represents the most precise
type knowledge we currently have about $[[e]]$, though as we will see below,
$[[ep]]$ may not exactly match the type we treat $[[e]]$ as having.  


\begin{figure}[H]

	\nonterms{T}
	\nonterms{Gamma}
	\nonterms{ep}

	\caption{Source Language Syntax: Types}
	\label{fig:type-syntax}
\end{figure}

\subsection{Static Semantics}
\label{subsec:type-rules}

\begin{figure}[H]
	\ottdefnHastype{}
	\caption{Source Language: Type Rules}
	\label{fig:source-typerules}
\end{figure}

The typing rules for our language are given in \autoref{fig:source-typerules}.
We assume that terms in this language are the result of a combined type-checking
and elaboration pass. Because of this, the typing rules are not syntax directed,
but mainly establish the safety of the language~\citep{agt}.
Once again, the typing rules are entirely standard, except for \rrule{HastypeAscr}.
This rule says that if $[[e]]$ has type $[[T2]]$, we give $[[ep e]]$ type $[[T1]]$
provided that $[[ep]]$ is evidence that $[[T1]]$ and $[[T2]]$ are \textit{consistent}.


We define consistency in terms of the precision meet operator: two types
are consistent provided that some third type is more precise than both of them (\autoref{fig:source-precision}).
We write $[[ep |- T1 ~=~ T2]]$ to mean that $[[ep]]$ is evidence of the consistency of $[[T1]]$ and $[[T2]]$.
Such a relationship holds whenever $[[ep]]$ is at least as precise as both $[[T1]]$ and $[[T2]]$.

The meet operator itself is defined in \autoref{fig:source-precision}. We wish $[[?]]$ to be consistent with any type,
so $[[?]]$ acts as an identity for the meet operator. The meet of $[[T]]$ with itself is $[[T]]$, and
the meet of function or arrow types is computed using the meet of the component types.
Note that this is not subtyping: there is no contravariance in the rule for arrow types.

Including a notion of consistency in our type system allows us to type terms that would be ill-typed
in a fully-static language.
For example, $[[1 + <<Nat>>(<<Bool>> true)]]$ is well-typed in our language: $[[empty |- true : Bool ]]$,
and $[[<<Bool>> |- Bool ~=~ ?]]$, so $[[empty |- <<Bool>>true : ? ]]$. Similarly, $[[<<Nat>> |- ? ~=~ Nat]]$,
so $[[empty |- <<Nat>>(<<Bool>>true) : Nat]]$, making the addition well-typed.

The meet operation gives us a means to combine different pieces of evidence for a value,
which allows evidence to evolve and gain precision as a program runs.
However, we also need operations on types, to extract typing information for particular
components of a type, such as the domain and codomain of a function. These operations are partial: they produce $[[?]]$ 
when given $[[?]]$, produce a result when given a type of the expected shape, and are undefined otherwise.
We define these operations in \autoref{fig:type-ops}.

We note that gradual typing usually begins with a definition of consistency, with precision and meet defined
in terms of consistency. Since GTFL is not a contribution of our work, we keep our presentation small
by defining operations in terms of meet. $[[T1]]$ and $[[T2]]$ are consistent if their meet exists,
and $[[T1]]$ is more precise than $[[T2]]$ if $[[T1 /\ T2 ==== T1]]$.


\begin{figure}[H]
	\ottdefnConsistent{}
	\ottdefnMeet{}
	\caption{Source Language: Type Consistency and Precision}
	\label{fig:source-precision}
\end{figure}


\begin{figure}[H]
	\begin{align*}
	& [[dom (T1 -> T1)]] & = && [[T1]] \\ 
	& [[dom ?]] & = && [[?]] \\
	& [[cod (T1 -> T1)]] & = && [[T2]] \\ 
	& [[cod ?]] & = && [[?]] \\
	& [[Prod @ i (T1 * T2)]] & = && [[Ti]] && i\in\{1,2\} \\ 
	& [[Prod @ i ?]] & = && [[?]] 
	\end{align*}
	\caption{Source Language: Partial Type Operations}
	\label{fig:type-ops} 
\end{figure}

\subsection{Runtime Semantics}

\begin{figure}[H]  
	\ottdefnsSemantics{} 
	\caption{Source Language: Small-Step Operational Semantics}
	\label{fig:source-semantics}
\end{figure}


As we saw above, $[[1 + <<Nat>>(<<Bool>> true)]]$ was assigned a type in our language. But how should such a term behave?
Allowing it to result in any integer value would require an arbitrary choice, so the only safe result
of such a computation is $[[error]]$. Specifically, because values may only contain one piece of top-level evidence, 
computation fails trying to combine the evidence objects $[[Bool]]$ and $[[Nat]]$, since their meet is undefined.

We present the full semantics for GTFL in \autoref{fig:source-semantics}. In general, we have rules which one expects in a static language,
plus rules accounting for values with evidence. When we have nested evidence it is combined with \rrule{RedAscr}. When applying a function
using the rule \rrule{RedAppEv},
we must first use domain information from the function's evidence to convert the argument to the type the function expects.
The result is then ascribed with the codomain information from the function's evidence.
These evidence operations mirror those of higher-order contracts~\citep{Findler:2002:CHF:581478.581484}.
We decompose the evidence for pairs in a similar way for projections in \rrule{RedProjEv}.

For primitive operations, we simply ignore evidence, as any well-typed values must have the appropriate type.
Similarly, in \rrule{RedAppEvPartial}, if we apply a function with evidence to a raw value, then we treat the argument as if it had been
ascribed evidence $[[?]]$.

If any of the evidence operations in the above rules are undefined, then the only way to preserve safety
is to step to $[[error]]$, which is what happens in \rrule{RedAppEvFail}, \rrule{RedProjFail} and \rrule{RedAscrFail}.
Context frames are defined in \autoref{fig:source-contexts}, and \rrule{RedContext} allows us to step within any context frame.
Similarly, errors are propagated with \rrule{RedContextFail}. The context rules establish a left-to-right, call-by-value
evaluation order.

While somewhat complex, basing a gradual language around evidence has several advantages.
First, the AGT approach allows us to take a pre-existing static language and introduce gradual types. 
Second, various properties of the language, such as type safety, hold by construction when using the evidence approach.



\begin{figure}[H] 
	\nonterms{C}
	\caption{Source Language: Context Frames}
	\label{fig:source-contexts}
\end{figure} 



\section{The Target Language}

Our target language, given in \autoref{fig:target-syntax}, is essentially an untyped version of the $\lambda^K$
 calculus presented by \citet{FtoTAL}.
 We have distinct syntactic classes for \textit{values} (metavariable $[[u]]$),
 and \textit{terms} (metavariable $[[t]]$). Each syntactic form for terms denotes
 a single operation on a value, and any nested computations must be explicitly
 represented with the passing of continuations.
 We do not provide a semantics for the target, but note that it is straightforward,
 using $\beta$-reductions, substitution for $\ottkw{let}$, and primitive operations in the usual way.
 
 Notably, our target language is \textit{not} gradual. Because gradual types let us write terms that have no purely static type,
 we treat our target as untyped typed. 

\begin{figure}[H]
	\nonterms{u}
	\nonterms{d}
	\nonterms{t}
	\caption{Target Language: Syntax}
	\label{fig:target-syntax}
\end{figure}

\section{The Translation} 

With our source and target languages defined, we can specify a translation between the two.
The key idea is that the evidence information in the source must be explicitly represented
using nested pairs (effectively untyped trees) and integer tags. While operations on evidence,
such as meet, were taken as atomic in the source, we provide explicit target implementations for these,
and to ensure safety, sequence these operations before the operations on values are performed.

\subsection{Translating Evidence}

In \autoref{fig:trans-types}, we translate each evidence object into a pair.
The first element of the pair is a tag, denoting the root type-constructor for the type.
We assume we have defined distinct integer constants $[[NAT]], [[BOOL]], [[ARR]], and [[PROD]]$.
For the simple types $[[Bool]]$, $[[Nat]]$ and $[[?]]$, we simply place a dummy value as the second pair element,
but for compound function and product types, we insert a pair containing the (recursively computed) representation
of the sub-components. 

\begin{figure}[H] TODO  
	\ottdefnEvTransform
	\caption{Translation: Evidence}
	\label{fig:trans-types}
\end{figure}

\subsubsection{Helper Functions}

With our evidence represented as tuples with integer tags, we must represent the partial functions
on types in our target language.
The implementation is given in \autoref{fig:meet-impl}.
Doing this is straightforward: if one argument is $[[?]]$,
then we return the other argument. Otherwise, we check if we have simple or complex types.
For simple types, either $[[Bool /\ Bool ==== Bool]]$, $[[Nat /\ Nat ==== Nat]]$.
For complex types, we check that the tags agree, then recursively compute the meets of the sub-components.
If neither argument is $[[?]]$, and there is a tag mismatch, then we must raise an exception, retuning the $[[error]]$
continuation.

For the partial functions decomposing types, we first check if the input is $[[?]]$,
in which case we return $[[?]]$. Otherwise, we check the tag, and if it is correct, we return
the relevant sub-component of the type. In all other cases, we throw an error.
We give an example implementation for $\ottkw{dom}$ in \autoref{fig:dom-impl}: either we are given $[[?]]$
and return $[[?]]$, we are given $[[T1 -> T2]]$ and we return $[[T1]]$, or we raise an exception.
We omit $\ottkw{cod}$, $\ottkw{proj}_1$ and $\ottkw{proj}_2$, but they are implemented similarly.

\begin{figure}[H]
	\begin{align*}
		 & [[MEET]] & = & [[fix self \ ty1 ty2 c . let tag1 := pi1 ty1 in let sub1 := pi2 ty1 in let isDyn1 := tag1 == DYN in  \\ ]]
		 &          &   & [[ if isDyn1 c[ty2]                                                                                  \\ ]]
		 &          &   & [[ let tag2 := pi1 ty2 in let sub2 := pi2 ty2 in let isDyn2 := tag2 == DYN in                        \\  ]]
		 &          &   & [[ if isDyn2 c[ty1]                                                                                  \\ ]]
		 &          &   & [[ let isNat1 := tag1 == NAT in let isNat2 := tag2 == NAT in                                         \\ ]]
		 &          &   & [[ if isNat1 (if isNat2 k[NAT] error)                                                                \\ ]]
		 &          &   & [[ let isBool1 := tag1 == BOOL in let isBool2 := tag2 == BOOL in                                     \\ ]]
		 &          &   & [[ if isBool1 (if isBool2 k[BOOL] error)                                                             \\ ]]
		 &          &   & [[ let isArrow1 := tag1 == ARR in let isArrow2 := tag2 == ARR in                                     \\ ]]
		 &          &   & [[ iif isArrow1                                                                                      \\ ]]
		 &          &   & [[ -- let dom1 := pi1 sub1 in let cod1 := pi2 sub1 in \\]]
		 &          &   & [[ -- iif isArrow2                                                                                   \\ ]]
		 &          &   & [[ -- -- let dom2 := pi1 sub2 in let cod2 := pi2 sub2 in \\]]
		 &          &   & [[ -- -- self [dom1, dom2, (\ meet1 . self[cod1, cod2, (\ meet2 . k[(ARR, (meet1, meet2))] )] )] ]]  \\
		 &          &   & [[ -- else error]]                                                                                   \\
		 &          &   & [[ let isProduct1 := tag1 == PROD in let isProduct2 := tag2 == PROD in                               \\ ]]
		 &          &   & [[ iif isProduct1                                                                                    \\ ]]
		 &          &   & [[ -- let lhs1 := pi1 sub1 in let rhs1 := pi2 sub1 in \\]]
		 &          &   & [[ -- iif isProduct2                                                                                 \\ ]]
		 &          &   & [[ -- -- let lhs2 := pi1 sub2 in let rhs2 := pi2 sub2 in \\]]
		 &          &   & [[ -- -- self [lhs1, lhs2, (\ meet1 . self[rhs1, rhs2, (\ meet2 . k[(PROD, (meet1, meet2))] )] )] ]] \\
		 &          &   & [[ -- else error]]                                                                                   \\
		 &          &   & [[ else error]]  
		%
	\end{align*}
	\caption{CPS implementation of meet}
	\label{fig:meet-impl}
\end{figure}

\begin{figure}[H]
	\begin{align*}
		 & [[DOM]] & = & [[\ ty c . let tag := pi1 ty1 in let sub := pi2 ty1 in let isDyn := tag == DYN in \\ ]]
		 &         &   & [[ if isDyn c[(DYN,0)]                                                            \\ ]]
		 &         &   & [[let isArrow := tag == ARR in                                                    \\ ]]
		 &         &   & [[if isArrow (let ret := pi1 sub in k[ret] ) error ]]
		%
	\end{align*}
	\caption{CPS implementation of domain}
	\label{fig:dom-impl}
\end{figure}

\subsection{Transforming Terms}

\begin{figure}[H]
	\ottdefnTransform
	\caption{Translation: Terms}  
	\label{fig:trans-terms}
\end{figure}





\section{Whole Program Correctness}

\subsection{Value Transformations}

Since we use small-step semantics, our reduction rules specify how to perform individual operations
on values, with context rules for performing nested computations. However, our translation does not distinguish
between, for example, a pair containing values and a pair containing reducible expressions.
In order to reason about the relationship between our reductions and our translation,
we define a special translation for values, which will aid in reasoning about how our translation
behaves when given values as input.

\begin{figure}[H]
	\ottdefnValTransform
	\caption{Translation: Values}
	\label{fig:trans-values}
\end{figure}

\subsection{Helper Lemmas}

We first show that our translation of evidence preserves the semantics of operations on evidence.

\begin{lemma}[Correctness of Evidence Translation]
	\label{lem:ev-correct}
	Consider evidence $[[ep]],[[ep']]$. Then, for any $[[k]]$:
	\begin{itemize}
		\item $[[MEET[ [|ep|], [|ep'|], k ] -->* k[ [| ep /\ ep' |] ] ]]$ if $[[ep /\ ep']]$ is defined.
		\item If $[[ep /\ ep']]$ is undefined, then $[[MEET[ [|ep|], [|ep'|] ] -->* error ]]$.
	\end{itemize}
	The same property holds for $[[dom ep]]$, $[[cod ep]]$, and $[[Prod@i ep]]$. 
\end{lemma}

Next, we show that our value translation always produces pairs. This is essential, since our source semantics
can distinguish between terms with and without evidence. However, since there is no explicit evidence in our target,
we represent all source values as evidence-value pairs in the target, with unascribed values simply
having the evidence $[[DYN]]$.  

\begin{lemma}[Canonical Forms for Translated Values]
	\label{lem:canonical-trans}
	For an irreducible $[[v]]$, $[[ [|v|] ==== ( [|ep|], u) ]] $ for some evidence $[[ep]]$ and CPS-value $[[u]]$.
	Moreover, if $[[v]]$ is a raw irreducible, then $[[ep]]=[[<<?>>]]$.
\end{lemma}
\begin{proof}
	By inversion on the definition of $[[ [|v|] ]]$.
\end{proof}

Next, we show that our value translation matches our term translation, after perhaps performing
some computation. This lemma is crucial for connecting the behaviour of small-step reduction rules,
which operate primarily on values, to the behaviour of the translations of these values.


\begin{lemma}[Value and Expression Translations Match]
	\label{lem:value-expr-trans}
	Let $[[v]]$ be an irreducible term. Then, for any $[[k]]$, $[[v]]$, $[[ [|v|]k -->* k [ [|v|] ]  ]]$.
\end{lemma}
\begin{proof}
	By induction on $[[v]]$. 

	\begin{itemize}
		\item Case $[[v]] = [[b]]$, $[[v]] = [[n]]$, or $[[v]] = [[\ x  . e]]$: trivial.
		\item Case $[[v]] = [[(v1, v2)]]$.
		      % By our hypothesis, for any $[[k0]]$, $[[ [|v1|]k0 -->* k0 [ [|v1|] ]  ]]$ and $[[ [|v2|]k0 -->* k0 [ [|v2|] ]  ]]$.
		      So $[[ [|(v1, v2)|]k ==== [| v1 |](\ x1 . [|v2|](\ x2 . k[(DYN, (x1, x2))] ) )  ]]$,
		      which, by our hypothesis, reduces to  $[[t1 -->* (\ x1 . (\ x2 . k[(DYN, (x1, x2))] )[ [|v2|] ] )[ [|v1|] ] ]]$,
		      which we can then reduce to $[[k[(DYN, ([|v1|],[|v2|]))] ]]$.
		\item Case $[[v]] = [[ep r]]$. Since all raw values are themselves irreducible,
		      our inductive hypothesis gives that
		      $[[ [|r|] (\ x . let x1 := pi1 x in let x2 := pi2 x in MEET [ [|ep|],x1, (\y . k [(y,x2)]) ] )]]$
		      steps to $[[(\ x . let x1 := pi1 x in let x2 := pi2 x in MEET [ [|ep|],x1, (\y . k [(y,x2)]) ] )[ [|r|] ] ]]$.
		      By \autoref{lem:canonical-trans}, $[[ [|r|] ]]$ is of the form
		      $[[(DYN, u)]]$ for some $[[u]]$.
		      So we can then $\beta$-reduce and apply the let-substitutions to reach $[[ MEET[ [|ep|], DYN, u ]  ]]$.
		      By \autoref{lem:ev-correct}, this steps to $[[([|ep|], u)]]$.
		      By the rule \rrule{TransformEv}, this means that $[[ [|ep r|] ]]$ also steps to this value.
 

	\end{itemize}
\end{proof}

Finally, we show that our translation preserves substitution, which is needed to show the correctness
of our translation of functions.

\begin{lemma}[Translation Commutes With Substitution]
	\label{lem:subst-commut}
	$[[ [|[x |=> v]e|]([x |=> [|v|] ]k) -->* [x |=> [|v|] ][|e|]k   ]]$.   
\end{lemma} 
\begin{proof}
	Follows from straightforward induction on $[[e]]$, combined with \autoref{lem:value-expr-trans}
	for the case where $[[e]]=[[x]]$.
\end{proof}

\subsection{Main Result}

These lemmas give us the tools we need to prove our main result.
We show that the translation takes a source term to a target term
that is \textit{equivalent} to the result of reduction 
Essentially, we are saying that if $[[e1 --> e2]]$, then $[[e1]]$ and $[[e2]]$
translate to target terms that will eventually step to some common term.

\begin{theorem}[Weak Simulation]

	If $[[e1 --> e2]]$, then for all $[[k]]$, $[[ [|e1|]k]] \equiv  [[ [|e2|]k  ]]$.

\end{theorem}
\begin{proof}
	We perform induction on the derivation tree of $[[e1 --> e2]]$.

	\begin{itemize}
		\item
		      \rrule{RedIfTrue}: then $[[e1]]=[[if true then e2 else e3]]$.
		      The translation $[[ [|true|]k' ==== k' [(DYN, true)]  ]]$ for any $[[k']]$,
		      so $[[ [|if true then e2 else e3|]k ]]$ is \\
		      ${[[(\ x0 . let x := pi2 x0 in if x ([|e2|]k) ([|e3|]k) ) [(DYN, true)]  ]]}$.
		      We can $\beta$-reduce to get $[[let x := pi2 (DYN, true) in if x [|e2|]k [|e3|]k ]]$,
		      and we can substitute $[[true]]$ for $[[x]]$ and reduce the $\ottkw{if}$ to get $[[ [|e2|]k ]]$.

		\item \rrule{RedIfFalse}: symmetric to RedIfTrue

		\item \rrule{RedIfEv}: $[[e1]]=[[if ep b then e2' else e3']]$. 
		      We know that $[[ [| b|]k' ==== k' [(DYN, b)] ]]$,
		      so $[[ [| ep b |]k'' ==== (\x. let x1 := pi1 x in let x2 := pi2 x in MEET[ [|ep|], x1, (\y.k''[(y,x2)]) ] )[(DYN,b)] ]]$.
		      We can $\beta$-reduce, and substitute with the let-expressions, to get
		      $[[(\x.  MEET[ [|ep|], DYN, (\y.k''[(y,b)]) ] )]]$.
		      However, $[[ ep /\ <<?>>]] = [[<<?>>]]$, so by \autoref{lem:ev-correct} this steps to $[[k'' [([|ep|], b)] ]]$.
		      Since the translation of $\ottkw{if}$ ignores any evidence in the condition,
		      we can use the same reasoning from RedIfTrue to show that it steps to $[[e2]]$ if $[[b]]$ is true, and $[[e3]]$ if $[[b]]$ is false.
 
		\item \rrule{RedApp}: then $[[e1]] = [[ (\ x  . e') v ]]$ and $[[e2]]=[[ [x |=> v]e' ]]$.
		We assume our terms follow variable convention so that $[[x]]$ is not free in $[[k]]$.

		Let $[[( [|ep|], u)]] = [[ [|v|] ]]$ (by \autoref{lem:canonical-trans}).
        If we apply \autoref{lem:value-expr-trans}, we can see that $[[ [|(\ x  . e') v|]k ]]$
        steps to \\$[[ (\ x1  x2. let y1 := pi1 x1 in ,,,)[(DYN, (\ x c . [|e'|]c )), ( [|ep|], u) ]  ]]$.
        We can $\beta$-reduce and apply the let-substitutions to then step to
        \\$[[ DOM [DYN, \y1' . COD [ DYN, \y1''. MEET [y1', [|ep|], (\ y3 . (\ x c . [|e'|]c ) [(y3, u), (\z3 . let z3' := pi1 z3 in let z3'' := pi2 z3 in MEET[y1'', z3', (\z4. k[(z4, z3'')] ) ] ) ] )] ] ] ]]$.
        By applying \autoref{lem:ev-correct} for $[[DOM]], [[COD]]$ and $[[MEET]]$ of $[[?]]$ respectively,
        we can step to  
        \\$[[  (\ x c . [|e'|]c ) [([|ep|], u), (\z3 . let z3' := pi1 z3 in let z3'' := pi2 z3 in MEET[DYN, z3', (\z4. k[(z4, z3'')] ) ] ) ] ]]$.
		This then $\beta$-reduces to 
		\\$[[  [ x |=> ([|ep|], u) ][|e'|](\z3 . let z3' := pi1 z3 in let z3'' := pi2 z3 in MEET[DYN, z3', (\z4. k[(z4, z3'')] ) ] ) ]]$.
		But, then, by \autoref{lem:ev-correct} and $\eta$-equivalence, this is equivalent to
		$[[  [ x |=> ([|ep|], u) ][|e'|]k  ]]$.
		But we know that this is $[[  [ x |=> [|v|] ][|e'|]k  ]]$ 
		Finally, our variable convention and \autoref{lem:subst-commut} give us that 
		this is equivalent to $[[ [| [x |=> v]e' |]k  ]]$.

		\item \rrule{RedAppEv}: then $[[e1]] = [[ ep1 (\ x  . e') ep2 v ]]$ and \\$[[e2]]=[[ cod ep1 ([x |=> (dom ep1 /\ ep2) v]e') ]]$. 
		Let $[[( [|ep2|], u)]] = [[ [|v|] ]]$ (by \autoref{lem:canonical-trans}).
        If we apply \autoref{lem:value-expr-trans}, we can see that $[[  [|ep1 (\ x . e') ep2 v|]k ]]$
        steps to \\$[[ (\ x1  x2. let y1 := pi1 x1 in ,,,)[( [|ep1|] , (\ x c . [|e'|]c )), ( [|ep2|], u) ]  ]]$.
        We can $\beta$-reduce and apply the let-substitutions to then step to
        \\$[[ DOM [ [|ep1|] , \y1' . COD [  [|ep1|] , \y1''. MEET [y1', [|ep2|], (\ y3 . (\ x c . [|e'|]c ) [(y3, u), (\z3 . let z3' := pi1 z3 in let z3'' := pi2 z3 in MEET[y1'', z3', (\z4. k[(z4, z3'')] ) ] ) ] )] ] ] ]]$.
        By applying \autoref{lem:ev-correct} for $[[DOM]], [[COD]]$ and $[[MEET]]$ of $[[?]]$ respectively,
        we can step to  
        \\$[[  (\ x c . [|e'|]c ) [([|dom ep1 /\ep2|], u), (\z3 . let z3' := pi1 z3 in let z3'' := pi2 z3 in \\(--MEET[ [|cod ep1|] , z3', (\z4. k[(z4, z3'')] ) ]) ) ] ]]$.
		\\This then $\beta$-reduces to 
		\\$[[  [ x |=> ([|dom ep1 /\ep2|], u) ][|e'|](\z3 . let z3' := pi1 z3 in let z3'' := pi2 z3 in \\(MEET[ [|cod ep1|] , z3', (\z4. k[(z4, z3'')] ) ]) ) ]]$.
		% But, then, by \autoref{lem:ev-correct} and $\eta$-equivalence, this is equivalent to
		% $[[  [ x |=> ([|dom ep1 /\ep2|], u) ][|e'|]k  ]]$.
		\\But, by the rule \rrule{TransformEv}, this is $\alpha$-equivalent to  $[[ [|cod ep1 ([x |=> (dom ep1 /\ ep2) v]e')|]k ]]$,
		giving us our result.

		\item \rrule{RedAppEvFail}: then $[[e1]] = [[ ep1 (\ x  . e') ep2 v ]]$ and $[[e2]]=[[ error ]]$.
		Let $[[( [|ep2|], u)]] = [[ [|v|] ]]$ (by \autoref{lem:canonical-trans}).
        If we apply \autoref{lem:value-expr-trans}, we can see that $[[  [|ep1 (\ x  . e') ep2 v|]k ]]$
        steps to \\$[[ (\ x1  x2. let y1 := pi1 x1 in ,,,)[( [|ep1|] , (\ x c . [|e'|]c )), ( [|ep2|], u) ]  ]]$.
        We can $\beta$-reduce and apply the let-substitutions to then step to
        \\$[[ DOM [ [|ep1|] , \y1' . COD [  [|ep1|] , \y1''. MEET [y1', [|ep2|], (\ y3 . (\ x c . [|e'|]c ) [(y3, u), (\z3 . let z3' := pi1 z3 in let z3'' := pi2 z3 in MEET[y1'', z3', (\z4. k[(z4, z3'')] ) ] ) ] )] ] ] ]]$.
        By applying \autoref{lem:ev-correct} for $[[MEET]]$ with our premise that $[[dom ep1 /\ ep2 undefined]]$
		we can step to  $[[error]]$.
		
		\item \rrule{RedAppEvPartial}: the same reasoning as \rrule{RedAppEv}, except that by \autoref{lem:value-expr-trans},
		we know that the argument's translation is annotated with $[[?]]$.

		\item \rrule{RedAppEvFailPartial}: the same reasoning as \rrule{RedfAppEvFail}, except for $[[DOM]]$ instead of $[[MEET]]$.
		\item \rrule{RedPlus}, \rrule{RedEqT}, \rrule{RedEqF}, \rrule{RedProj}: trivial.
		\item \rrule{RedPlusEvL}, \rrule{RedPlusEvR}, \rrule{RedEqEvL}, \rrule{RedEqEvR}: follows from our induction hypothesis, combined with the fact that the \rrule{TransformPlus} and \rrule{TransformEq} both ignore evidence.
		
		\item \rrule{RedProj} Then $[[e1]]=[[pi @ i (v1 , v2)]]$ and $[[e2]] = [[vi]]$. 
		Then combining \rrule{TransformProj} with \autoref{lem:canonical-trans} and \autoref{lem:value-expr-trans},
		we have $[[ [|pi @ i (v1 , v2)|]k  ]]$ steps to $[[(\ x . let y1 := pi1 x in ,,,)[(DYN, ([|v1|], [|v2|] ) ) ] ]]$.
		If we $\beta$-reduce and substitute for the let-expressions, we get $[[PROD @ i [DYN, (\ z1 . ,,,)] ]]$.
		We apply \autoref{lem:ev-correct} and let-substitution to step to $[[ MEET[DYN, u1, (\z1' . k [(z1', u2)] )] ]]$,
		where $[[ [|vi|] ]] = [[(u1, u2)]]$ by \autoref{lem:canonical-trans}.
		By \autoref{lem:ev-correct} this steps to $[[ k [(u1, u2)] ]]$
		 which we can also step to
		from $[[ [|vi|]k ]]$ by \autoref{lem:value-expr-trans}.

		\item \rrule{RedProjEv} Then $[[e1]]=[[pi @ i (ep (v1 , v2))]]$ and $[[e2]] = [[(Prod@i ep) vi]]$. 
		Then combining \rrule{TransformProj} with \autoref{lem:canonical-trans} and \autoref{lem:value-expr-trans},
		we have $[[ [|pi @ i (v1 , v2)|]k  ]]$ steps to \\$[[ (\ x . let y1 := pi1 x in ,,,)[ ( [|ep|], ([|v1|], [|v2|] ))] ]]$.
		If we $\beta$-reduce, and substitute for the let-expressions, we get $[[ PROD @ i [ [|ep|], (\ z1 . ,,,)] ]]$.
		We apply \autoref{lem:ev-correct} and let-substitution to step to $[[ MEET[ [|Prod @ i ep|], u1, (\z1' . k [(z1', u2)] )] ]]$,
		where $[[ [|vi|] ]] = [[(u1, u2)]]$ by \autoref{lem:canonical-trans}.

		Looking now at $[[e2]]$, we can apply \rrule{TransformEv} and \autoref{lem:canonical-trans}, then $\beta$-reduce
		and substitute
		to see that $[[ [|(Prod@i ep) vi|]k ]]$ also steps to $[[ MEET[ [|Prod @ i ep|], u1, (\z1' . k [(z1', u2)] )] ]]$.   
		
		\item \rrule{RedProjFail}
		Then $[[e1]]=[[pi @ i (ep (v1 , v2))]]$ and $[[e2]] = [[error]]$. 
		Then combining \rrule{TransformProj} with \autoref{lem:canonical-trans} and \autoref{lem:value-expr-trans},
		we have $[[ [|pi @ i (v1 , v2)|]k  ]]$ steps to \\ $[[ (\ x . let y1 := pi1 x in ,,,)[ ( [|ep|], ([|v1|], [|v2|] ))] ]]$.
		If we $\beta$-reduce, and substitute for the let-expressions, we get $[[ PROD @ i [ [|ep|], (\ z1 . ,,,)] ]]$.
		We then apply \autoref{lem:ev-correct} and let-substitution to step to $[[error]]$. 
 
		\item \rrule{RedAscr}
		Then $[[e1]]=[[ep1 (ep2 r)]]$ and $[[e2]]=[[(ep1 /\ ep2) r]]$.
		Applying \rrule{TransformEv} with \autoref{lem:canonical-trans} and and \autoref{lem:value-expr-trans}, we can see that
		this steps to $[[MEET[ [|ep1|], [|ep2|], (\ y . k[(y, u2) ]) ] ]]$
		where $[[ [|ep2 r|] ]] = [[ ([|ep2|], u2) ]]$. By \autoref{lem:ev-correct}, this steps to $[[ k [([| ep1 /\ ep2 |], u2)] ]]$,
		which we can also step to from $[[ [|(ep1 /\ ep2) r|]k ]]$ by \autoref{lem:value-expr-trans}.


		\item \rrule{RedAscrFail}
		Then $[[e1]]=[[ep1 (ep2 r)]]$ and $[[e2]]=[[error]]$.
		Applying \rrule{TransformEv} with \autoref{lem:canonical-trans} and and \autoref{lem:value-expr-trans}, we can see that
		this steps to $[[MEET[ [|ep1|], [|ep2|], (\ y . k[(y, u2) ]) ] ]]$
		where $[[ [|ep2 r|] ]] = [[ ([|ep2|], u2) ]]$. By \autoref{lem:ev-correct} along with our premise, this steps to $[[error]]$.

		\item \rrule{RedContext}: Then $[[e1]]=[[C[e1'] ]] $ and $[[e2]]=[[C[e2'] ]] $ where $[[e1' --> e2']]$.
		By our hypothesis, $[[ [|e1|]k ]] \equiv [[ [|e2|]k ]]$ for any $[[k]]$.
 
		Suppose that $[[C]]$ is one of $[[ __ e]]$, $[[(__, e)]]$, $[[pi1 __]]$, $[[pi2 __]]$, $[[ep __]]$,
		$[[__ + e]]$, $[[__ == e]]$ or $[[if __ then e3 else e4]]$. These are the cases where the hole is the ``first''
		sub-expression. 
		In each case, there exists some $[[k']]$ such that $[[ [|C[e1'] |]k ==== [|e1'|]k' ]]$ and 
		$[[ [|C[e2'] |]k ==== [|e2'|]k' ]]$. By our hypothesis, these terms are equal.
		
		The remaining cases are when the first sub-expression is already a value, and the context frame hole is the second sub-expression.
		In these cases, there exists some $[[v]]$ (the first sub-expression) and $[[k']]$
		such that $[[ [|C[e1'] |]k ==== [|v|](\x . [|e1'|]k' ) ]]$ and $[[ [|C[e2'] |]k ==== [|v|](\x . [|e2'|]k' ) ]]$.
		We assume the bound variable $[[x]]$ is fresh, that is, it does not occur in $[[e1']]$ or $[[e2']]$.
		We can apply \autoref{lem:value-expr-trans} to show that these step to $[[ [x |=> [|v|] ][|e1'|]k' ]]$
		and $[[ [x |=> [|v|] ][|e2'|]k' ]]$ respectively. \autoref{lem:subst-commut} and our freshness assumption 
		shows that these are equivalent to
		$[[ [| e1' |]( [x |=> [|v|] ]k ) ]]$ and $[[ [| e2' |]([x |=> [|v|] ]k') ]]$ respectively.
		Finally, our hypothesis shows that these two terms are equivalent.

		\item \rrule{RedContextFail}: then $[[e1]]=[[C[e1'] ]] $ and $[[e2]]=[[error]] $ where $[[e1' --> error]]$.
		By our hypothesis, $[[ [|e1'|]k ]] \equiv [[ error ]]$ for any $[[k]]$.

		Suppose that $[[C]]$ is one of $[[ __ e]]$, $[[(__, e)]]$, $[[pi1 __]]$, $[[pi2 __]]$, $[[ep __]]$,
		$[[__ + e]]$, $[[__ == e]]$ or $[[if __ then e3 else e4]]$. These are the cases where the hole is the ``first''
		sub-expression. 
		In each case, there exists some $[[k']]$ such that $[[ [|C[e1'] |]k ==== [|e1'|]k' ]]$. By our hypothesis,
		this steps to $[[error]]$.

		
		The remaining cases are when the first sub-expression is already a value, and the context frame hole is the second sub-expression.
		In these cases, there exists some $[[v]]$ (the first sub-expression) and $[[k']]$
		such that $[[ [|C[e1'] |]k ==== [|v|](\x . [|e1'|]k' ) ]]$ .
		We assume the bound variable $[[x]]$ is fresh, that is, it does not occur in $[[e1']]$.
		We can apply \autoref{lem:value-expr-trans} to show that this steps to $[[ [x |=> [|v|] ][|e1'|]k' ]]$.
		 \autoref{lem:subst-commut} and our freshness assumption 
		show that this is equivalent to
		$[[ [| e1' |]([x |=> [|v|] ]k') ]]$, which, by our hypothesis,
		steps to $[[error]]$.
		

		  
	\end{itemize}
\end{proof}

The conventional whole-program correctness theorem follows from this directly.
If we take natural numbers as observables, we note that if $[[t]]\equiv[[n]]$, then $[[t -->* n]]$,
since $[[n]]$ cannot reduce further. Induction on the number of source-reduction steps gives us whole-program correctness.

\begin{corollary}[Whole Program Correctness] 
	If ${eval([[e]])=n}$, then ${eval[[ ([|e|](\ x . halt [x]) )]]=[[halt [ [|n|] ] ]]}$.
\end{corollary}
 
\section{Breaking Full Abstraction}

Unfortunately, our translation does not preserve contextual equivalence of source programs.
Consider $[[(\ x . x + 1)]] : [[Nat -> Nat]]$ and $[[(\ x . <<Nat>>x + 1)]] : [[Nat -> Nat]]$.
The type rules of our language ensures that any value substituted for $[[x]]$ must be a number,
so annotating $[[x]]$ with $[[<<Nat>>]]$ has no effect.
However, in our translation, such an annotation is translated into a meet operation between the annotation
of $[[x]]$ and $[[NAT]]$. In the context $\square([[(TAG, n)]])$, where $[[TAG]]$ is not $[[NAT]]$ or $[[DYN]]$,
this meet operation will fail, cause the second expression to produce $[[error]]$ while the first succeeds.
 

\bibliography{myrefs}  

\end{document}
